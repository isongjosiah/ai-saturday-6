{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Support Vector Machines\n", "In this notebook, you will classify emails as either spam or not spam using support vector machines. You are given a dataset of labeled emails, where the labels are 1 if they are ham (not spam), and -1 if they are spam. The lines of the emails have already been slightly processed, such that different words are space delimited, however little other processing has occurred. \n", "\n", "## Preliminary notes\n", "1. You cannot use scikit-learn. \n", "2. As we move into more advanced algorithms and techniques, there will be more introductions of randomness. This means that some of the example outputs in the notebook contain some randomness, and will probably not match your results exactly. Verify your code by checking your properties/invariants or feeding in static inputs for which you can calculate the output. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import gzip\n", "import math\n", "import numpy as np\n", "import scipy.sparse as sp\n", "import scipy.optimize\n", "from collections import Counter\n", "from testing.testing import test"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_data(emails_filepath, labels_filepath):\n", "    with gzip.open(emails_filepath, 'rt') as f:\n", "        emails = f.readlines()\n", "    with gzip.open(labels_filepath, 'rt') as f:\n", "        labels = np.loadtxt(f)\n", "    return emails, labels"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Here we provide a wrapper function to cache the loaded dataset. This helps it not take quite as long to re-run. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["global DATA_CACHE\n", "DATA_CACHE = None\n", "\n", "def get_data():\n", "    global DATA_CACHE\n", "    if DATA_CACHE is None:\n", "        DATA_CACHE = load_data('X1.txt.gz', 'y1.txt.gz')\n", "    return DATA_CACHE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You will make use of the tf-idf matrix from the previous assignment. We have provided you the function below."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def tfidf(docs):\n", "    all_words = set([a for a in \" \".join(docs).split(\" \") if a != \"\"])\n", "    all_words_dict = {k:i for i,k in enumerate(all_words)}\n", "    word_counts = [Counter([a for a in d.split(\" \") if a != \"\"]) for d in docs]\n", "\n", "    data = [a for wc in word_counts for a in wc.values()]\n", "    rows = [i for i,wc in enumerate(word_counts) for a in wc.values()]\n", "    cols = [all_words_dict[k] for wc in word_counts for k in wc.keys()]\n", "    X = sp.coo_matrix((data, (rows,cols)), (len(docs), len(all_words)))\n", "\n", "    idf = np.log(float(len(docs))/np.asarray((X > 0).sum(axis=0))[0])\n", "    return X * sp.diags(idf), list(all_words)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## SVM classification\n", "Recall the support vector machine (SVM) from slide 17 of linear classification. It a straightforward algorithm, but there are some performance- and sparsity-related caveats.\n", "\n", "### Specification\n", "1. If you do not use matrix operations, your code will be **very slow**. Every function in here can be implemented in 1 or 2 lines using matrix equations, and the only for loop you need is the training loop for gradient descent. **If your code is slow here, it will be extremely slow in the next section when doing parameter search**.\n", "2. You should train your SVM using gradient descent as described in the slides. \n", "3. Since this is a convex function, your gradient steps should always decrease your objective. A simple check when writing these optimization procedures is to print your objectives and verify that this is the case (or plot them with matplotlib).\n", "4. You can also use scipy.optimize.check_grad to numerically verify the correctness of your gradients.\n", "5. For the unlikely boundary case where your hypothesis outputs 0, we will treat that as a positive prediction. \n", "6. Be careful of numpy.matrix objects which are constrained to always have dimension 2 (scipy operations will sometimes return this instead of an ndarray). \n", "7. The exact objective function to implement is: $$\n", "\\mathcal L(y, X, \\theta, \\lambda) = \\text{sum}\\left\\{\\text{max}\\{1 - y\\cdot \\theta^T X, 0 \\}\\right\\} + \\frac\\lambda2(\\theta\\cdot\\theta)\n", "$$ where $X$ is the input, $y$ is the true label, $\\theta$ is the model weight, and $\\lambda$ is the regularization weight. The inner `max` operation is performed over each element, and the outer `sum` operation returns the sum of the elements of the vector.\n", "8. The gradient is:$$\n", "    v = \\mathbb1\\left\\{y* (X \\theta)) <= 1\\right\\}\n", "\\\\    \n", "\\\\    \\frac{d\\theta}{d\\mathcal L} = \\text{sum}_\\text{col}\\{-y* v*X\\} + \\lambda\\theta\n", "$$\n", "where the $\\text{sum}_\\text{col}$ operation calculates the sum over each column, producing a vector the same shape as $\\theta$.\n", "\n", "### Hints:\n", "\n", "1. There are a few different types of multiplication involved in this: \n", "    - matrix-vector,\n", "    - vector-vector dot product,\n", "    - scalar-vector product,\n", "    - vector-vector _element-wise_ product,\n", "    - a vector-matrix _row-wise_ product (where each element of a vector multiplies the corresponding row of a matrix) \n", "2. Keep track of the shape of each intermediate product! It will help you as you implement this.\n", "3. Read the math above and notice how we have restructured the multiplication.\n", "4. In the gradient calculation:\n", "\n", "    - $X$ has as many rows as the length of $y$, and as many columns as the length of $\\theta$\n", "    - $v$ is a vector containing only ones and zeros, the same size as $y$; \n", "    - $-y*v$ is a vector the same length as $y$\n", "    - $(-y*v)*X$ is the same size as $X$.\n", "5. If these equations don't make sense, you should consult the slides; the same equations are written sample-by-sample there.\n", "\n", "Some useful tricks for debugging: \n", "\n", "1. Use very simple inputs (i.e. small vectors of ones) and compare the output of each function with a hand calculation. \n", "2. One way to guarantee your gradient is correct is to verify it numerically using a derivative approximation. You can read more about numerical differentiation methods here (https://en.wikipedia.org/wiki/Finite_difference) but for your purposes, you can use scipy.optimize.check_grad to do the numerical checking for you. \n", "\n", "Given the specifications, fill out the `SVM` class below."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def SVM_test(SVM_impl):\n", "    # Use small random inputs and outputs for initial debugging\n", "    y0 = np.random.randint(0, 2, 5) * 2 - 1\n", "    X0 = np.random.random((5, 10))\n", "    t0 = np.random.random(10)\n", "    svm0 = SVM_impl(X0, y0, 1e-4)\n", "    svm0.theta = t0\n", "\n", "    def obj(theta):\n", "        svm0.theta = theta\n", "        return svm0.objective(svm0.X, svm0.y)\n", "\n", "    def grad(theta):\n", "        svm0.theta = theta\n", "        return svm0.gradient()\n", "\n", "    # Numerically verify the correctness of your gradients\n", "    test.true(scipy.optimize.check_grad(obj, grad, t0) < 1e-6)\n", "    \n", "    # Check your gradient steps decrease your objective\n", "    pretrain_objective = svm0.objective(svm0.X, svm0.y)\n", "    svm0.train(verbose=True)\n", "    train_objective = svm0.objective(svm0.X, svm0.y)\n", "    test.true(pretrain_objective > train_objective)\n", "\n", "    # Try training your SVM on tfidf features using the labeled email dataset\n", "    emails, labels = get_data()\n", "    features, all_words = tfidf(emails)\n", "    svm = SVM_impl(features, labels, 1e-4)\n", "    svm.train(verbose=True)\n", "    \n", "    # Test for perfect training classification accuracy\n", "    yp = svm.predict(features)\n", "    test.equal(np.mean(yp*labels < 0), 0.0)\n", "\n", "    # Uncomment the line below to see how long your training takes to run. \n", "    # Our solution takes ~1.1 seconds for 100 iterations\n", "    %timeit svm.train()\n", "\n", "\n", "@test\n", "class SVM:\n", "    def __init__(self, X, y, reg):\n", "        \"\"\" Initialize the SVM attributes and initialize the weights vector to the zero vector. \n", "            Attributes: \n", "                X (array_like) : training data intputs\n", "                y (vector) : 1D numpy array of training data outputs\n", "                reg (float) : regularizer parameter\n", "                theta : 1D numpy array of weights\n", "        \"\"\"\n", "        self.X = X\n", "        self.y = y\n", "        self.reg = reg\n", "        self.theta = np.zeros(X.shape[1])\n", "    \n", "    def objective(self, X, y):\n", "        \"\"\" Calculate the objective value of the SVM. When given the training data (self.X, self.y), this is the \n", "            actual objective being optimized. \n", "            Args:\n", "                X (array_like) : array of examples, where each row is an example\n", "                y (array_like) : array of outputs for the training examples\n", "            Output:\n", "                (float) : objective value of the SVM when calculated on X,y\n", "        \"\"\"\n", "        pass\n", "    \n", "    def gradient(self):\n", "        \"\"\" Calculate the gradient of the objective value on the training examples. \n", "            Output:\n", "                (vector) : 1D numpy array containing the gradient\n", "        \"\"\"\n", "        pass\n", "    \n", "    def train(self, niters=100, learning_rate=1, verbose=False):\n", "        \"\"\" Train the support vector machine with the given parameters. \n", "            Args: \n", "                niters (int) : the number of iterations of gradient descent to run\n", "                learning_rate (float) : the learning rate (or step size) to use when training\n", "                verbose (bool) : an optional parameter that you can use to print useful information (like objective value)\n", "        \"\"\"\n", "        pass\n", "\n", "    def predict(self, X):\n", "        \"\"\" Predict the class of each label in X. \n", "            Args: \n", "                X (array_like) : array of examples, where each row is an example\n", "            Output:\n", "                (vector) : 1D numpy array containing predicted labels\n", "        \"\"\"\n", "        pass"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Model Selection: Cross validation and Parameter Grid Search \n", "As you may have noticed, there are parameters in the SVM learning algorithm that we chose somewhat arbitrarily: the regularization parameter and the learning rate (also technically the number of iterations for the learning algorithm, but you'll only consider the first two for simplicity). \n", "\n", "We were also able to achieve perfect training accuracy with these random parameters. This should make you suspicious: we have an enormous amount of features so it would be extremely easy to overfit to the data, so our model may not generalize well. \n", "\n", "You will now evaluate and select parameters using cross validation and grid search."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## K-fold cross validation\n", "How can we evaluate our choice of parameters? One way is to perform k-fold cross validation, which operates as follows \n", "\n", "1. We split the data into k+1 randomly selected but uniformly sized pieces, and set aside one block for testing.\n", "2. For each of the remaining k parts, we train the model on k-1 parts and validate our model on the heldout part. \n", "3. This gives k results, and the average of these runs gives the final result.\n", "\n", "The idea is that by holding out part of the dataset as validation data, we can train and measure our generalization ability. Note the key fact here: the training does not see the validation data at all, which is why it measures generalization! Randomizing the groups removes bias from ordering (i.e. if these results occurred in chronological order, we don't want to train on only Monday's results to predict on Wednesday's results), and averaging over the groups reduces the variance. \n", "\n", "In this problem, we will use classification error rate as our result metric (so the fraction of times in which our model returns the wrong answer). Calculating this value via k-fold cross validation gives us a measure of how well our model generalizes to new data (lower error rate is better). \n", "\n", "### Specification\n", "You will be filling out the `ModelSelector` class below. \n", "1. Implement the `__init__` function to populate the attributes `blocks` and `test_block`. Break the examples in k+1 groups as follows: \n", "    * break the permutation into blocks of size $\\text{ceil}\\left(\\frac{n}{k+1}\\right)$ (the last block may be shorter than the rest)\n", "    * set aside the k+1th group as the testing block, and use the remaining k blocks for cross validation\n", "    * use the permutation as indices to select the rows that correspond to that block\n", "    * Example: `k=2`, `P=[1,3,2,4,5,6]` sets aside `[5,6]` as the test set, and break the remaining permutation into `[[1,3],[2,4]]` so the blocks of data for validation are `X[[1,3],:]` and `X[[2,4],:]`\n", "    * the order of the indices in the blocks should match the order in the original permutation\n", "2. Implement the `cross_validation` function. For each group k, train the model on all other datapoints, and compute the error rate on the held-out group. Return the average error rate over all k folds. \n", "\n", "Try running this on the tfidf features. Can you achieve the same performance on the validation dataset as you did on the training dataset? Remember to use a random permutation (you'll get noticeably different results).\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def ModelSelector_test(ModelSelector_impl):\n", "    emails, labels = get_data()\n", "    features, all_words = tfidf(emails)\n", "\n", "    # Assign random perturbation\n", "    P = np.random.permutation(features.shape[0])\n", "\n", "    # Check for correct block and test block attributes\n", "    MS = ModelSelector_impl(features, labels, P, 5, 100, SVM)\n", "    test.equal(len(MS.blocks), 5)\n", "    test.true(np.all(MS.blocks[0] == P[:len(MS.blocks[0])]))\n", "\n", "    # Check cross validation error\n", "    cv_err = MS.cross_validation(0.5, 0.1)\n", "    test.true(cv_err < 0.05)\n", "\n", "    # Perform grid search on learning rates and regularization\n", "    lr, reg = MS.grid_search(np.array([0.001, 1., 10.]), np.array([0.0001, 0.1, 1.]))\n", "\n", "    # Check correct selection of learning rate and regularization parameter\n", "    test.equal(lr, 1.0)\n", "    test.equal(reg, 0.1)\n", "\n", "    # Cross validation/SVM errors\n", "    test.true(MS.cross_validation(0.001, 0.0001) - 0.00924 < 1e-2)\n", "    test.true(MS.cross_validation(0.001, 0.1) - 0.00924 < 1e-2)\n", "    test.true(MS.cross_validation(0.001, 1.0) - 0.00924 < 1e-2)\n", "    test.true(MS.cross_validation(1.0, 0.0001) - 0.00924 < 1e-2)\n", "    test.true(MS.cross_validation(1.0, 0.1) - 0.00900 < 1e-2)\n", "    test.true(MS.cross_validation(1.0, 1.0) - 0.79388 < 1e-2)\n", "    test.true(MS.cross_validation(10.0, 0.0001) - 0.00912 < 1e-2)\n", "    test.true(MS.cross_validation(10.0, 0.1) - 0.79388 < 1e-2)\n", "    test.true(MS.cross_validation(10.0, 1.0) - 0.80972 < 1e-2)\n", "\n", "    # Check test error\n", "    err, _ = MS.test(lr,reg)\n", "    test.true(err < 0.05)\n", "\n", "\n", "@test\n", "class ModelSelector:\n", "    \"\"\" A class that performs model selection. \n", "        Attributes:\n", "            blocks (list) : list of lists of indices of each block used for k-fold cross validation, e.g. blocks[i] \n", "            gives the indices of the examples in the ith block \n", "            test_block (list) : list of indices of the test block that used only for reporting results\n", "            \n", "    \"\"\"\n", "    def __init__(self, X, y, P, k, niters, SVM_impl):\n", "        \"\"\" Initialize the model selection with data and split into train/valid/test sets. Split the permutation into blocks \n", "            and save the block indices as an attribute to the model. \n", "            Args:\n", "                X (array_like) : array of features for the datapoints\n", "                y (vector) : 1D numpy array containing the output labels for the datapoints\n", "                P (vector) : 1D numpy array containing a random permutation of the datapoints\n", "                k (int) : number of folds\n", "                niters (int) : number of iterations to train for\n", "                SVM_impl: reference to your SVM class\n", "        \"\"\"\n", "        pass\n", "\n", "    def cross_validation(self, lr, reg):\n", "        \"\"\" Given the permutation P in the class, evaluate the SVM using k-fold cross validation for the given parameters \n", "            over the permutation\n", "            Args: \n", "                lr (float) : learning rate\n", "                reg (float) : regularizer parameter\n", "            Output: \n", "                (float) : the cross validated error rate\n", "        \"\"\"\n", "        pass\n", "    \n", "    def grid_search(self, lrs, regs):\n", "        \"\"\" Given two lists of parameters for learning rate and regularization parameter, perform a grid search using\n", "            k-wise cross validation to select the best parameters. \n", "            Args:  \n", "                lrs (list) : list of potential learning rates\n", "                regs (list) : list of potential regularizers\n", "            Output: \n", "                (lr, reg) : 2-tuple of the best found parameters\n", "        \"\"\"\n", "        pass\n", "    \n", "    def test(self, lr, reg):\n", "        \"\"\" Given parameters, calculate the error rate of the test data given the rest of the data. \n", "            Args: \n", "                lr (float) : learning rate\n", "                reg (float) : regularizer parameter\n", "            Output: \n", "                (err, svm) : tuple of the error rate of the SVM on the test data and the learned model\n", "        \"\"\"\n", "        pass"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Grid search\n", "Now, we have a means of evaluating our choice of parameters. We can now combine this with a grid search over parameters to determine the best combination. Given two lists of parameters, we compute the classification error using k-fold cross validation for each pair of parameters, and output the parameters that produces the best validation result. Our solution takes ~ 1 minute and 45 seconds to run a grid search on learning rates [0.1, 1, 10] and regularization [0.01, 0.1, 1, 10] with 100 iterations and k=4.\n", "\n", "### Specification\n", "You will implement the following functions in the `ModelSelector` class. \n", "1. Implement the `grid_search` function. Given two lists of parameters for the learning rate and regularization, perform a grid search using k-wise cross validation to select the best parameters, and return the best found parameters.\n", "2. Implement the `test` function. Given the best found parameters, train a new model using all the training and validation data, and return the classification accuracy on the test data."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Feature Compression \n", "While you are able to get decent results using an SVM and basic tf-idf features, there are 2 main problems here:\n", "1. The actual dataset is 8x larger than the one that you load at the start\n", "2. The number of features is extremely bloated and consumes a lot of space and computing power for a binary classification problem\n", "\n", "So the above methodology would actually take a lot of time and memory to run on the full dataset. Following the example you did in the text classification notebook, we would need to save the tf-idf matrix for the entire training dataset (which is enormous), and then use that to generate features on new examples. \n", "\n", "One way to tackle this is to generate fewer, but effective, features. For example, instead of generating full tf-idf features for every single word in every email, we can instead try to focus on keywords that frequently only occur in spam email.\n", "\n", "Fill in the `find_frequent_indicator_words` function below.\n", "\n", "Specification:\n", "\n", "  - Split each document `d` into words using `d.split()`. No further preprocessing is required.\n", "  - Determine the frequency with which words occur in spam and ham emails. Count each appearance of each word (even multiple appearances in a document).\n", "  - Select words that only ever appear in spam and words that only ever appear in ham\n", "  - From these words, return sets of those that appear at least `threshold` times."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def find_frequent_indicator_words_test(find_frequent_indicator_words_impl):\n", "    emails, labels = get_data()\n", "    s,h = find_frequent_indicator_words_impl(emails, labels, 50)\n", "    test.equal(len(s), 2422)\n", "    test.equal(len(h), 290)\n", "    test.true('advertisement' in s)\n", "    test.true('Stephanie' in h)\n", "\n", "@test\n", "def find_frequent_indicator_words(docs, y, threshold):\n", "    \"\"\" Given a list of emails and corresponding labels, return a list of frequent indicator words \n", "        for spam emails and for ham emails. \n", "        Args:  \n", "            emails (list) : list of strings\n", "            labels (list) : list of integers\n", "            threshold (int): frequency threshold\n", "        Output: \n", "            (spam_frequent_words, ham_frequent_words) : 2-tuple of frequent indicator words for each class\n", "    \"\"\"\n", "    pass"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Efficient Spam Detection\n", "\n", "Your goal here is to get at least 80% accuracy on spam detection in an efficient manner. You will use the frequent indicator words implemented above and generate 2 features per email: the number of spam indicator words and the number of ham indicator words for a total of two features. This is a huge dimensionality reduction!\n", "\n", "Given the frequent indicator words, you will first generate 2 features per email for a given training dataset. Then, you will use your ModelSelector to perform a grid search and train your SVM on the training dataset. Finally, we will evaluate your trained SVM on a separate test set, where you aim to achieve at least 80% accuracy.  \n", "\n", "Begin by filling out the `emails2features` function below. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def email2features_test(email2features_impl):\n", "    # Get frequent indicator words on dataset we've been using so far\n", "    emails, labels = get_data()\n", "    frequent_indictator_words = find_frequent_indicator_words(emails, labels, 50)\n", "\n", "    # Generate features for a different set of emails using the frequent indicator words\n", "    emails_tr, _ = load_data('Xtr_spam.txt.gz', 'ytr_spam.txt.gz')\n", "    features = email2features_impl(emails_tr, frequent_indictator_words)\n", "\n", "    test.equal(features.shape, (1000, 2))\n", "    test.equal(list(features[0]), [64., 0.])\n", "    test.equal(list(features[888]), [0., 1])\n", "\n", "\n", "    \n", "@test\n", "def email2features(emails, frequent_indicator_words):\n", "    \"\"\" Given a list of emails, create a matrix containing features for each email.\n", "        Args: \n", "            emails (list) : list of strings\n", "            frequent_indicator_words (tuple) : tuple containing (list of spam frequent words, \n", "                                               list of ham frequent words)\n", "        Output: \n", "            features : matrix containing features for each email where matrix shape is (num_emails, num_features)\n", "    \"\"\"\n", "    pass"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now you will fill out the `efficient_spam_detection` function below. You will first use your `emails2features` function to generate features for the given set of emails. Then, you will use your ModelSelector to perform a grid search over the learning rate and regularization parameters, and train your SVM using the chosen parameters on the given training dataset. Finally, we will evaluate your trained SVM on a separate test set, where you aim to achieve at least 80% accuracy. Note that you choose the value of k for cross-validation, the set of parameters to evaluate on, as well as the number of iterations to train for. Choose these values so that you still get at least 80% accuracy but also so that you pass the time constraints of Diderot."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def efficient_spam_detection_test(efficient_spam_detection_impl):\n", "    emails, labels = get_data()\n", "    emails_tr, labels_tr = load_data('Xtr_spam.txt.gz', 'ytr_spam.txt.gz')\n", "\n", "    frequent_indicator_words = find_frequent_indicator_words(emails, labels, 50)    \n", "    svm = efficient_spam_detection_impl(emails_tr, labels_tr, frequent_indicator_words)\n", "    \n", "    # Training error\n", "    features = email2features(emails_tr, frequent_indicator_words)\n", "    yp = svm.predict(features)\n", "    print(np.mean(yp * labels_tr < 0))\n", "    test.true(np.mean(yp * labels_tr < 0) <= 0.2)\n", "\n", "    \n", "@test\n", "def efficient_spam_detection(emails, labels, frequent_indicator_words):\n", "    \"\"\" Given a training dataset of emails and labels, use the given frequent indicator words to\n", "        generate features per email. Use your ModelSelector to perform a grid search and train your SVM\n", "        on the training dataset, and return the trained SVM. \n", "        Args: \n", "            emails (list) : list of strings\n", "            labels (list): list of integers\n", "            frequent_indicator_words (tuple) : tuple containing (list of spam frequent words, \n", "                                               list of ham frequent words)\n", "        Output: \n", "            svm (SVM): SVM instance\n", "    \"\"\"\n", "    pass"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.7"}}, "nbformat": 4, "nbformat_minor": 1}